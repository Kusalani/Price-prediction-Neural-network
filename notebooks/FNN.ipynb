{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 11:09:54.352577: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 11:09:54.384002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-29 11:09:54.384044: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-29 11:09:54.384082: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-29 11:09:54.390993: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 11:09:54.391899: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-29 11:09:55.308369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"../data/data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a datetime object\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to separate numerical features (e.g., year, month, day)\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "\n",
    "# Drop the original 'Date' column\n",
    "data = data.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GemstoneName</th>\n",
       "      <th>Color</th>\n",
       "      <th>Clarity</th>\n",
       "      <th>Cut</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spinel</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1.36</td>\n",
       "      <td>9000</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat's Eye</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Translucent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18000</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinel</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1.36</td>\n",
       "      <td>9000</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat's Eye</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Translucent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18000</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapphire</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Emerald</td>\n",
       "      <td>0.73</td>\n",
       "      <td>20000</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>Red</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Emerald</td>\n",
       "      <td>0.51</td>\n",
       "      <td>30000</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>Red</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Emerald</td>\n",
       "      <td>0.95</td>\n",
       "      <td>30000</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>0.73</td>\n",
       "      <td>30000</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Oval</td>\n",
       "      <td>0.73</td>\n",
       "      <td>30000</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>Red</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>Emerald</td>\n",
       "      <td>0.95</td>\n",
       "      <td>30500</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GemstoneName   Color      Clarity      Cut  Weight  Price  Year  Month  \\\n",
       "0         Spinel   Brown  Transparent     Oval    1.36   9000  2021      1   \n",
       "1      Cat's Eye  Yellow  Translucent     Oval    0.75  18000  2021      1   \n",
       "2         Spinel   Brown  Transparent     Oval    1.36   9000  2021      1   \n",
       "3      Cat's Eye  Yellow  Translucent     Oval    0.75  18000  2021      1   \n",
       "4       Sapphire  Purple  Transparent  Emerald    0.73  20000  2021      2   \n",
       "..           ...     ...          ...      ...     ...    ...   ...    ...   \n",
       "696         Ruby     Red  Transparent  Emerald    0.51  30000  2022      5   \n",
       "697         Ruby     Red  Transparent  Emerald    0.95  30000  2022      8   \n",
       "698         Ruby  Purple  Transparent     Oval    0.73  30000  2022      9   \n",
       "699         Ruby  Purple  Transparent     Oval    0.73  30000  2023      1   \n",
       "700         Ruby     Red  Transparent  Emerald    0.95  30500  2022      3   \n",
       "\n",
       "     Day  \n",
       "0     15  \n",
       "1     15  \n",
       "2     17  \n",
       "3     28  \n",
       "4     12  \n",
       "..   ...  \n",
       "696   11  \n",
       "697    4  \n",
       "698   20  \n",
       "699   17  \n",
       "700   21  \n",
       "\n",
       "[701 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables (e.g., GemstoneName, Color, Clarity, Cut)\n",
    "label_encoders = {}\n",
    "for column in ['GemstoneName', 'Color', 'Clarity', 'Cut']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = data.drop(columns=['Price'])\n",
    "y = data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[['Weight']] = scaler.fit_transform(X[['Weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Add an input layer\n",
    "model.add(keras.layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add hidden layers\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 17ms/step - loss: 9880010752.0000 - mae: 65647.3672 - val_loss: 9572652032.0000 - val_mae: 65291.1758\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9692542976.0000 - mae: 64470.5156 - val_loss: 9364030464.0000 - val_mae: 63920.2266\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 9438983168.0000 - mae: 63064.7578 - val_loss: 9049596928.0000 - val_mae: 62073.9492\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 9060588544.0000 - mae: 61191.2852 - val_loss: 8580209152.0000 - val_mae: 59358.7734\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8525004288.0000 - mae: 58632.0781 - val_loss: 7973162496.0000 - val_mae: 56014.1133\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7884240384.0000 - mae: 55641.2227 - val_loss: 7239305728.0000 - val_mae: 52146.9961\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 7125874688.0000 - mae: 52974.3398 - val_loss: 6524088832.0000 - val_mae: 49738.8164\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 6449913344.0000 - mae: 51358.5664 - val_loss: 5906166784.0000 - val_mae: 49508.2539\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5960468480.0000 - mae: 52004.1133 - val_loss: 5506511360.0000 - val_mae: 51211.3789\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5672660992.0000 - mae: 53409.9531 - val_loss: 5360077824.0000 - val_mae: 53278.7188\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5587620352.0000 - mae: 54922.5625 - val_loss: 5324922368.0000 - val_mae: 54685.4414\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5578020864.0000 - mae: 55914.4531 - val_loss: 5319846912.0000 - val_mae: 55472.1484\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5577225728.0000 - mae: 56210.7734 - val_loss: 5319733248.0000 - val_mae: 55412.9453\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5576139264.0000 - mae: 56082.2773 - val_loss: 5319803392.0000 - val_mae: 55299.8711\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5576033792.0000 - mae: 56083.8086 - val_loss: 5319402496.0000 - val_mae: 55402.7500\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5577411584.0000 - mae: 56234.3047 - val_loss: 5319260160.0000 - val_mae: 55388.4336\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5575244800.0000 - mae: 55924.6406 - val_loss: 5320478208.0000 - val_mae: 55051.2305\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5576216576.0000 - mae: 55966.1523 - val_loss: 5319318016.0000 - val_mae: 55247.3125\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5581046272.0000 - mae: 56276.2031 - val_loss: 5318681600.0000 - val_mae: 55468.7930\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5577361408.0000 - mae: 55938.5039 - val_loss: 5320340992.0000 - val_mae: 54998.5000\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5575569920.0000 - mae: 56070.3203 - val_loss: 5318359040.0000 - val_mae: 55479.3125\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5574625280.0000 - mae: 56327.1953 - val_loss: 5318418944.0000 - val_mae: 55684.0547\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5579556352.0000 - mae: 56588.7422 - val_loss: 5318626304.0000 - val_mae: 55796.9062\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5573979136.0000 - mae: 56266.9844 - val_loss: 5317870592.0000 - val_mae: 55461.1914\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5574249472.0000 - mae: 56034.1680 - val_loss: 5319277056.0000 - val_mae: 55033.0000\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5575729664.0000 - mae: 55859.8945 - val_loss: 5318621184.0000 - val_mae: 55114.0586\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5572743680.0000 - mae: 56129.2344 - val_loss: 5317443072.0000 - val_mae: 55576.6602\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5576409088.0000 - mae: 56509.7344 - val_loss: 5317778944.0000 - val_mae: 55777.7656\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5576204288.0000 - mae: 56529.8555 - val_loss: 5317311488.0000 - val_mae: 55677.1836\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5578944000.0000 - mae: 55974.9844 - val_loss: 5317821440.0000 - val_mae: 55148.0547\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5575019008.0000 - mae: 55980.4414 - val_loss: 5316846080.0000 - val_mae: 55414.6172\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5573465600.0000 - mae: 56181.4883 - val_loss: 5316960768.0000 - val_mae: 55288.9727\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5570994688.0000 - mae: 55964.3242 - val_loss: 5316781568.0000 - val_mae: 55299.5586\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5573864448.0000 - mae: 55937.7305 - val_loss: 5316659200.0000 - val_mae: 55290.9844\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5579083264.0000 - mae: 56482.2305 - val_loss: 5318119424.0000 - val_mae: 56017.4531\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5568796160.0000 - mae: 56426.2500 - val_loss: 5316042752.0000 - val_mae: 55475.5039\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5568370688.0000 - mae: 56005.6562 - val_loss: 5316953088.0000 - val_mae: 55108.1758\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5574685696.0000 - mae: 56125.5664 - val_loss: 5316178944.0000 - val_mae: 55248.4023\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5572142080.0000 - mae: 55772.7969 - val_loss: 5317400064.0000 - val_mae: 54990.4609\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5569763328.0000 - mae: 55850.0195 - val_loss: 5316314112.0000 - val_mae: 55148.5977\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5571072512.0000 - mae: 55841.8672 - val_loss: 5316195840.0000 - val_mae: 55142.2227\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5566521856.0000 - mae: 56135.1172 - val_loss: 5315763712.0000 - val_mae: 55780.4336\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5569776128.0000 - mae: 56459.6055 - val_loss: 5315579904.0000 - val_mae: 55771.7734\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5571070464.0000 - mae: 56250.7539 - val_loss: 5314871296.0000 - val_mae: 55522.6211\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5572978688.0000 - mae: 56004.4375 - val_loss: 5315644416.0000 - val_mae: 55130.1914\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5571401216.0000 - mae: 55863.5781 - val_loss: 5315096064.0000 - val_mae: 55216.4414\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5569029120.0000 - mae: 56165.2422 - val_loss: 5314967552.0000 - val_mae: 55764.0195\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5568038400.0000 - mae: 56442.7695 - val_loss: 5314641408.0000 - val_mae: 55713.8711\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5568766976.0000 - mae: 56379.2109 - val_loss: 5314155008.0000 - val_mae: 55542.1602\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5571124736.0000 - mae: 55874.0195 - val_loss: 5315935744.0000 - val_mae: 54961.1094\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 5315935744.0000 - mae: 54961.1094\n",
      "Test Mean Absolute Error: 54961.11\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Mean Absolute Error: {test_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model.save('gemstone_price_prediction_model_FNN.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gemstone_name_label_encoder.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bpsychic-winner-rqv4v46pgjhxgg4/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Load label encoders\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bpsychic-winner-rqv4v46pgjhxgg4/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m label_encoders \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bpsychic-winner-rqv4v46pgjhxgg4/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m label_encoders[\u001b[39m'\u001b[39m\u001b[39mGemstoneName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mgemstone_name_label_encoder.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bpsychic-winner-rqv4v46pgjhxgg4/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m label_encoders[\u001b[39m'\u001b[39m\u001b[39mColor\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mcolor_label_encoder.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bpsychic-winner-rqv4v46pgjhxgg4/workspaces/Price-prediction-Neural-network/notebooks/FNN.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m label_encoders[\u001b[39m'\u001b[39m\u001b[39mClarity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mclarity_label_encoder.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gemstone_name_label_encoder.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = keras.models.load_model('gemstone_price_prediction_model_FNN.h5')\n",
    "\n",
    "# Function to preprocess user input\n",
    "def preprocess_input(user_input):\n",
    "    # Extract user input\n",
    "    date = pd.to_datetime(user_input['date'])\n",
    "    gemstone_name = user_input['gemstone_name']\n",
    "    color = user_input['color']\n",
    "    clarity = user_input['clarity']\n",
    "    weight = float(user_input['weight'])  # Ensure 'weight' is converted to a float\n",
    "\n",
    "    # Create a DataFrame for prediction\n",
    "    new_data = pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'GemstoneName': [gemstone_name],\n",
    "        'Color': [color],\n",
    "        'Clarity': [clarity],\n",
    "        'Cut': [0],  # Since 'Cut' was not included in user input, you can assign a default value or handle it differently\n",
    "        'Weight': [weight]\n",
    "    })\n",
    "\n",
    "    # Normalize the numerical feature\n",
    "    new_data[['Weight']] = scaler.transform(new_data[['Weight']])\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# User input\n",
    "user_input = {\n",
    "    'date': input('Enter the date (e.g., 2023-10-29): '),\n",
    "    'gemstone_name': input('Enter the gemstone name: '),\n",
    "    'color': input('Enter the color: '),\n",
    "    'clarity': input('Enter the clarity: '),\n",
    "    'weight': input('Enter the weight: ')\n",
    "}\n",
    "\n",
    "# Preprocess user input\n",
    "new_data = preprocess_input(user_input)\n",
    "\n",
    "# Make a price prediction\n",
    "predicted_price = loaded_model.predict(new_data)\n",
    "print(f\"Predicted Price: ${predicted_price[0][0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
